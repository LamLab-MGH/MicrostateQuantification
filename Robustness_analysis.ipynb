{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MC-AOrR-4e9C"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr, wasserstein_distance\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.metrics import mutual_info_score\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import combinations\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score, adjusted_rand_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import norm\n",
    "from scipy import stats\n",
    "import re\n",
    "\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import warnings\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "def ep_read(filename):\n",
    "    \"\"\"\n",
    "    Read data from a Cartool .ep file and convert it to a numpy array.\n",
    "\n",
    "    Parameters:\n",
    "    filename (str): Path to the file to be read.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: A 2D array containing the data from the file, with each line\n",
    "                   converted to a list of floats.\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as file:\n",
    "        data = np.array([list(map(float, line.split())) for line in file if line.strip()])\n",
    "    return data\n",
    "\n",
    "def calculate_spatial_correlation(v1, v2):\n",
    "    \"\"\"\n",
    "    Calculate the Pearson correlation coefficient between two vectors, ignoring NaN values.\n",
    "\n",
    "    Parameters:\n",
    "    v1 (array-like): First vector.\n",
    "    v2 (array-like): Second vector.\n",
    "\n",
    "    Returns:\n",
    "    float: The Pearson correlation coefficient between the non-NaN elements of v1 and v2.\n",
    "    \"\"\"\n",
    "    return pearsonr(v1[~np.isnan(v1)], v2[~np.isnan(v2)])[0]\n",
    "\n",
    "def calculate_inverse_emd(v1, v2):\n",
    "    \"\"\"\n",
    "    Calculate the \"inverse\" of the normalized Earth Mover's Distance (EMD) (1-EMD) between two vectors.\n",
    "\n",
    "    Parameters:\n",
    "    v1 (array-like): First vector.\n",
    "    v2 (array-like): Second vector.\n",
    "\n",
    "    Returns:\n",
    "    float: The inverse of the normalized EMD, ranging from 0 to 1.\n",
    "           1 indicates identical distributions, 0 indicates maximally different distributions.\n",
    "    \"\"\"\n",
    "    v1_clean = v1[~np.isnan(v1)].ravel()\n",
    "    v2_clean = v2[~np.isnan(v2)].ravel()\n",
    "    emd = wasserstein_distance(v1_clean, v2_clean)\n",
    "\n",
    "    min_val = min(np.min(v1_clean), np.min(v2_clean))\n",
    "    max_val = max(np.max(v1_clean), np.max(v2_clean))\n",
    "    max_emd = wasserstein_distance([min_val]*len(v1_clean), [max_val]*len(v2_clean))\n",
    "\n",
    "    if max_emd == 0:\n",
    "        normalized_emd = 0\n",
    "    else:\n",
    "        normalized_emd = emd / max_emd\n",
    "\n",
    "    return 1 - normalized_emd\n",
    "\n",
    "def calculate_mutual_information(v1, v2):\n",
    "    \"\"\"\n",
    "    Calculate the normalized mutual information between two vectors.\n",
    "\n",
    "    Parameters:\n",
    "    v1 (array-like): First vector.\n",
    "    v2 (array-like): Second vector.\n",
    "\n",
    "    Returns:\n",
    "    float: The normalized mutual information, ranging from 0 to 1.\n",
    "           Higher values indicate stronger statistical dependence between the vectors.\n",
    "    \"\"\"\n",
    "    v1_clean = v1[~np.isnan(v1)].ravel()\n",
    "    v2_clean = v2[~np.isnan(v2)].ravel()\n",
    "\n",
    "    discretizer = KBinsDiscretizer(n_bins=20, encode='ordinal', strategy='uniform', subsample=None)\n",
    "    discretizer.fit(v1_clean.reshape(-1, 1))\n",
    "    v1_discretized = discretizer.fit_transform(v1_clean.reshape(-1, 1)).ravel()\n",
    "    v2_discretized = discretizer.fit_transform(v2_clean.reshape(-1, 1)).ravel()\n",
    "\n",
    "    mi = mutual_info_score(v1_discretized, v2_discretized)\n",
    "\n",
    "    h1 = mutual_info_score(v1_discretized, v1_discretized)\n",
    "    h2 = mutual_info_score(v2_discretized, v2_discretized)\n",
    "    return mi / min(h1, h2)\n",
    "\n",
    "def calculate_difference(map1, map2):\n",
    "    \"\"\"\n",
    "    Calculate a difference metric.\n",
    "\n",
    "    Parameters:\n",
    "    map1 (numpy.ndarray): First map (A).\n",
    "    map2 (numpy.ndarray): Second map (B).\n",
    "    scale (float): Scaling factor to adjust the sensitivity of the metric.\n",
    "\n",
    "    Returns:\n",
    "    float: Difference metric that is 1 when A = B and approaches 0 when A and B differ significantly.\n",
    "    \"\"\"\n",
    "    # Calculate the absolute difference\n",
    "    difference = np.abs(map2 - map1)\n",
    "\n",
    "    # Use an exponential decay function to create the metric\n",
    "    metric = np.exp(-difference)\n",
    "\n",
    "    return np.nanmean(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w47SMeXtqPlw"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from scipy import stats\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import csv\n",
    "\n",
    "def plot_clusters_2d_tsne(folder_paths, n_clusters, output_path='.', max_iter=1000, n_perturbations=10, noise_levels=np.linspace(0, 1, 10)):\n",
    "    \"\"\"\n",
    "    Plot 2D t-SNE representation of microstate clusters for multiple folders, perform second-level (group) clustering,\n",
    "    calculate clustering robustness using noise perturbations, and save metrics to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    folder_paths (list): List of paths to the folders containing .ep files.\n",
    "    n_clusters (int): Number of clusters for K-means clustering.\n",
    "    output_path (str, optional): Path to save the output plot and CSV. Defaults to current directory.\n",
    "    max_iter (int, optional): Maximum number of iterations for t-SNE. Defaults to 1000.\n",
    "    n_perturbations (int, optional): Number of noise perturbations for analysis. Defaults to 10.\n",
    "    noise_levels (np.array, optional): Array of noise levels for perturbation analysis. Defaults to np.linspace(0, 1, 10).\n",
    "    \"\"\"\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\", message=\"KMeans is known to have a memory leak on Windows with MKL\")\n",
    "\n",
    "    np.random.seed(42)\n",
    "\n",
    "    all_ari_scores = []\n",
    "    all_ari_confidence_intervals = []\n",
    "    folder_labels = []\n",
    "    reference_centroids = None\n",
    "\n",
    "    output_ep_folder = os.path.join(output_path, 'Output_ep')\n",
    "    os.makedirs(output_ep_folder, exist_ok=True)\n",
    "\n",
    "    # Dictionary to store metrics for CSV\n",
    "    csv_data = {}\n",
    "\n",
    "    def align_clusters(centroids, reference_centroids):\n",
    "        correlation_matrix = np.corrcoef(centroids, reference_centroids)\n",
    "        correlation_submatrix = correlation_matrix[:n_clusters, n_clusters:]\n",
    "        row_ind, col_ind = linear_sum_assignment(-correlation_submatrix)\n",
    "        return col_ind\n",
    "\n",
    "    for idx, folder_path in enumerate(folder_paths):\n",
    "        # Only select files that match the pattern ending with \".ep\"\n",
    "        files = [f for f in os.listdir(folder_path) if re.search(r'.ep$', f)]\n",
    "\n",
    "        subject_files = {}\n",
    "        for file in files:\n",
    "            subject_id = file.split('.')[1]  # Assumes the subject ID is the second element in the split by '.'\n",
    "            if subject_id not in subject_files:\n",
    "                subject_files[subject_id] = []\n",
    "            subject_files[subject_id].append(file)\n",
    "\n",
    "        all_subject_centroids = []\n",
    "        all_subject_labels = []\n",
    "        all_subject_microstates = []\n",
    "\n",
    "        folder_name = os.path.basename(folder_path)\n",
    "\n",
    "        for subject_id, subject_file_list in subject_files.items():\n",
    "            subject_microstates = [ep_read(os.path.join(folder_path, file)) for file in subject_file_list]\n",
    "\n",
    "            # Check that all microstates have the same shape\n",
    "            microstate_shapes = [microstate.shape for microstate in subject_microstates]\n",
    "            if len(set(microstate_shapes)) > 1:\n",
    "                print(f\"Warning: Microstates for subject {subject_id} in folder {folder_name} have inconsistent shapes: {microstate_shapes}\")\n",
    "                continue  # Skip this subject or handle it differently (e.g., pad or interpolate the shapes)\n",
    "\n",
    "            # If all shapes are consistent, proceed\n",
    "            subject_microstates = np.array(subject_microstates)\n",
    "\n",
    "            n_samples, n_microstates, n_channels = subject_microstates.shape\n",
    "            subject_microstates_reshaped = subject_microstates.reshape(n_samples * n_microstates, n_channels)\n",
    "\n",
    "            kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "            subject_labels = kmeans.fit_predict(subject_microstates_reshaped)\n",
    "            subject_centroids = kmeans.cluster_centers_\n",
    "\n",
    "            if reference_centroids is None:\n",
    "                reference_centroids = subject_centroids\n",
    "                reference_labels = subject_labels\n",
    "            else:\n",
    "                aligned_indices = align_clusters(subject_centroids, reference_centroids)\n",
    "                subject_centroids = subject_centroids[aligned_indices]\n",
    "                subject_labels = np.array([aligned_indices[label] for label in subject_labels])\n",
    "\n",
    "            all_subject_centroids.append(subject_centroids)\n",
    "            all_subject_labels.append(subject_labels)\n",
    "            all_subject_microstates.append(subject_microstates)\n",
    "\n",
    "            # Calculate Rand scores for perturbations\n",
    "            for noise_level in noise_levels:\n",
    "                rand_scores_for_level = []\n",
    "                for _ in range(n_perturbations):\n",
    "                    noise = np.random.normal(0, noise_level, subject_microstates_reshaped.shape)\n",
    "                    perturbed_data = subject_microstates_reshaped + noise\n",
    "                    perturbed_labels = KMeans(n_clusters=n_clusters, n_init=10, random_state=42).fit_predict(perturbed_data)\n",
    "                    rand = adjusted_rand_score(subject_labels, perturbed_labels)\n",
    "                    rand_scores_for_level.append(rand)\n",
    "                # Store metrics in csv_data\n",
    "                if subject_id not in csv_data:\n",
    "                    csv_data[subject_id] = {}\n",
    "                csv_data[subject_id][f'Rand_{folder_name}_{noise_level:.2f}'] = np.mean(rand_scores_for_level)\n",
    "\n",
    "            # Save subject centroids as .ep file\n",
    "            centroid_filename = f'{folder_name}_{subject_id}_centroids.ep'\n",
    "            centroid_filepath = os.path.join(output_ep_folder, centroid_filename)\n",
    "            np.savetxt(centroid_filepath, subject_centroids, fmt='%.6f')\n",
    "            print(f\"Saved centroids for subject {subject_id} in {folder_name} to: {centroid_filepath}\")\n",
    "\n",
    "        all_subject_centroids = np.array(all_subject_centroids)\n",
    "        all_subject_labels = np.array(all_subject_labels)\n",
    "        all_subject_microstates = np.array(all_subject_microstates)\n",
    "\n",
    "        # Perform second-level clustering on the organized centroids\n",
    "        organized_centroids = all_subject_centroids.reshape(-1, n_channels)\n",
    "        second_kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "        second_level_labels = second_kmeans.fit_predict(organized_centroids)\n",
    "        second_level_centroids = second_kmeans.cluster_centers_\n",
    "\n",
    "        # Save second-level centroids as .ep file\n",
    "        second_level_filename = f'{folder_name}_second_level_centroids.ep'\n",
    "        second_level_filepath = os.path.join(output_ep_folder, second_level_filename)\n",
    "        np.savetxt(second_level_filepath, second_level_centroids, fmt='%.6f')\n",
    "        print(f\"Saved second-level centroids for {folder_name} to: {second_level_filepath}\")\n",
    "\n",
    "        # Compute mean distances between subject centroids and second-level centroids\n",
    "        subject_mean_distances = {}\n",
    "        for i, subject_id in enumerate(subject_files.keys()):\n",
    "            subject_centroids = all_subject_centroids[i]\n",
    "            distances = np.linalg.norm(subject_centroids - second_level_centroids, axis=1)\n",
    "            mean_distance = np.mean(distances)\n",
    "            subject_mean_distances[subject_id] = mean_distance\n",
    "\n",
    "        # Identify subjects with closest, farthest, and median mean distances\n",
    "        subject_ids = list(subject_mean_distances.keys())\n",
    "        mean_distances = np.array([subject_mean_distances[sid] for sid in subject_ids])\n",
    "\n",
    "        # Sort the subjects by mean distance\n",
    "        sorted_indices = np.argsort(mean_distances)\n",
    "        closest_index = sorted_indices[0]\n",
    "        farthest_index = sorted_indices[-1]\n",
    "        median_index = sorted_indices[len(sorted_indices) // 2]\n",
    "\n",
    "        # Assign fixed labels: Subject 1, Subject 2, Subject 3\n",
    "        subject1_id = subject_ids[closest_index]\n",
    "        subject2_id = subject_ids[median_index]\n",
    "        subject3_id = subject_ids[farthest_index]\n",
    "\n",
    "        # Create a mapping from subject IDs to labels and markers\n",
    "        highlighted_subjects = {\n",
    "            subject1_id: {'label': 'Subject 1', 'marker': '^'},  # Triangle Up\n",
    "            subject2_id: {'label': 'Subject 2', 'marker': 's'},  # Square\n",
    "            subject3_id: {'label': 'Subject 3', 'marker': 'D'}   # Diamond\n",
    "        }\n",
    "\n",
    "        # Save their centroids as .ep files with labels indicating their fixed subject numbers\n",
    "        for subject_id, info in highlighted_subjects.items():\n",
    "            index = subject_ids.index(subject_id)\n",
    "            subject_centroids = all_subject_centroids[index]\n",
    "            centroid_filename = f'{folder_name}_{subject_id}_{info[\"label\"].lower().replace(\" \", \"_\")}_centroids.ep'\n",
    "            centroid_filepath = os.path.join(output_ep_folder, centroid_filename)\n",
    "            np.savetxt(centroid_filepath, subject_centroids, fmt='%.6f')\n",
    "            print(f\"Saved centroids for {info['label']} ({subject_id}) in {folder_name} to: {centroid_filepath}\")\n",
    "\n",
    "        # Perform t-SNE on all subject centroids and second-level centroids\n",
    "        all_centroids = np.vstack([organized_centroids, second_level_centroids])\n",
    "        tsne = TSNE(n_components=2, perplexity=8, max_iter=max_iter, random_state=42)\n",
    "        reduced_centroids = tsne.fit_transform(all_centroids)\n",
    "\n",
    "        # Separate the reduced centroids\n",
    "        reduced_subject_centroids = reduced_centroids[:-n_clusters]\n",
    "        reduced_second_level_centroids = reduced_centroids[-n_clusters:]\n",
    "\n",
    "        # Plot centroids for each subject and second-level centroids\n",
    "        fig, ax = plt.subplots(figsize=(12, 10), dpi=100)\n",
    "        cluster_colors = ['r', 'g', 'b', 'y']\n",
    "        cluster_names = ['A', 'B', 'C', 'D']\n",
    "\n",
    "        # Map subject IDs to indices for quick access\n",
    "        subject_id_to_index = {sid: idx for idx, sid in enumerate(subject_ids)}\n",
    "\n",
    "        # Keep track of labels added to the legend to avoid duplicates\n",
    "        legend_labels = set()\n",
    "\n",
    "        # Prepare legend order\n",
    "        legend_entries = []\n",
    "\n",
    "        # Plot highlighted subjects in desired order\n",
    "        for subject_id in [subject1_id, subject2_id, subject3_id]:\n",
    "            info = highlighted_subjects[subject_id]\n",
    "            index = subject_id_to_index[subject_id]\n",
    "            subject_reduced_centroids = reduced_subject_centroids[index*n_clusters:(index+1)*n_clusters]\n",
    "            label_text = info['label']\n",
    "            marker_style = info['marker']\n",
    "            marker_size = 500\n",
    "            alpha = 1.0\n",
    "            # Plot and label the centroids\n",
    "            for j in range(n_clusters):\n",
    "                legend_label = f'{label_text} Centroid {cluster_names[j]}'\n",
    "                if legend_label not in legend_labels:\n",
    "                    sc = ax.scatter(subject_reduced_centroids[j, 0], subject_reduced_centroids[j, 1],\n",
    "                                    c=cluster_colors[second_level_labels[index*n_clusters+j]], marker=marker_style, s=marker_size, edgecolors='k',\n",
    "                                    label=legend_label)\n",
    "                    legend_entries.append(sc)\n",
    "                    legend_labels.add(legend_label)\n",
    "                else:\n",
    "                    ax.scatter(subject_reduced_centroids[j, 0], subject_reduced_centroids[j, 1],\n",
    "                               c=cluster_colors[second_level_labels[index*n_clusters+j]], marker=marker_style, s=marker_size, edgecolors='k')\n",
    "            # No annotation inside the plot\n",
    "\n",
    "        # Plot other subjects without annotations or legend entries\n",
    "        for i, subject_id in enumerate(subject_ids):\n",
    "            if subject_id in highlighted_subjects:\n",
    "                continue\n",
    "            subject_reduced_centroids = reduced_subject_centroids[i*n_clusters:(i+1)*n_clusters]\n",
    "            marker_style = 'o'  # Circle marker for other subjects\n",
    "            marker_size = 100\n",
    "            alpha = 0.5\n",
    "            # Plot without annotation\n",
    "            for j in range(n_clusters):\n",
    "                ax.scatter(subject_reduced_centroids[j, 0], subject_reduced_centroids[j, 1],\n",
    "                           c=cluster_colors[second_level_labels[i*n_clusters+j]], marker=marker_style, s=marker_size, alpha=alpha)\n",
    "\n",
    "        # Plot second-level centroids and add to legend\n",
    "        for j in range(n_clusters):\n",
    "            legend_label = f'Second-level Centroid {cluster_names[j]}'\n",
    "            if legend_label not in legend_labels:\n",
    "                sc = ax.scatter(reduced_second_level_centroids[j, 0], reduced_second_level_centroids[j, 1],\n",
    "                                c=cluster_colors[j], marker='*', s=500, edgecolors='k', linewidth=2,\n",
    "                                label=legend_label)\n",
    "                legend_entries.append(sc)\n",
    "                legend_labels.add(legend_label)\n",
    "            else:\n",
    "                ax.scatter(reduced_second_level_centroids[j, 0], reduced_second_level_centroids[j, 1],\n",
    "                           c=cluster_colors[j], marker='*', s=500, edgecolors='k', linewidth=2)\n",
    "\n",
    "        ax.set_title(f'{folder_name} - Highlighted Subject Centroids', fontsize=20)\n",
    "        ax.set_xlabel('t-SNE Component 1', fontsize=18)\n",
    "        ax.set_ylabel('t-SNE Component 2', fontsize=18)\n",
    "        ax.tick_params(axis='both', labelsize=14)\n",
    "\n",
    "        # Arrange legend entries in desired order\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        # Create a mapping from label to handle\n",
    "        label_to_handle = dict(zip(labels, handles))\n",
    "        # Desired legend order\n",
    "        desired_labels = []\n",
    "        for subject_label in ['Subject 1', 'Subject 2', 'Subject 3']:\n",
    "            for cluster_name in cluster_names:\n",
    "                desired_labels.append(f'{subject_label} Centroid {cluster_name}')\n",
    "        for cluster_name in cluster_names:\n",
    "            desired_labels.append(f'Second-level Centroid {cluster_name}')\n",
    "        # Filter handles and labels based on desired order\n",
    "        handles = [label_to_handle[label] for label in desired_labels if label in label_to_handle]\n",
    "        labels = [label for label in desired_labels if label in label_to_handle]\n",
    "        ax.legend(handles, labels, fontsize=12, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_path, f'{folder_name}_highlighted_subject_centroids_1s.png'), dpi=100, bbox_inches='tight')\n",
    "        print(f\"Saved highlighted subject centroids plot for {folder_name} to: {os.path.join(output_path, f'{folder_name}_highlighted_subject_centroids_1s.png')}\")\n",
    "        plt.close()\n",
    "\n",
    "        # Perform robustness analysis on the second-level clustering\n",
    "        ari_scores = []\n",
    "        ari_confidence_intervals = []\n",
    "        for noise_level in noise_levels:\n",
    "            ari_scores_for_level = []\n",
    "            for _ in range(n_perturbations):\n",
    "                noise = np.random.normal(0, noise_level, organized_centroids.shape)\n",
    "                perturbed_data = organized_centroids + noise\n",
    "                perturbed_labels = KMeans(n_clusters=n_clusters, n_init=10, random_state=42).fit_predict(perturbed_data)\n",
    "                ari = adjusted_rand_score(second_level_labels, perturbed_labels)\n",
    "                ari_scores_for_level.append(ari)\n",
    "\n",
    "            mean_ari = np.mean(ari_scores_for_level)\n",
    "            ari_scores.append(mean_ari)\n",
    "\n",
    "            # Calculate confidence interval using standard error\n",
    "            se = stats.sem(ari_scores_for_level)\n",
    "            ci = (mean_ari - 1.96 * se, mean_ari + 1.96 * se)\n",
    "            ari_confidence_intervals.append(ci)\n",
    "\n",
    "        all_ari_scores.append(ari_scores)\n",
    "        all_ari_confidence_intervals.append(ari_confidence_intervals)\n",
    "        folder_labels.append(folder_name)\n",
    "\n",
    "    # Robustness plot for ARI\n",
    "    fig, ax_robustness = plt.subplots(figsize=(10, 10), dpi=100)\n",
    "    for idx, (ari_scores, ari_confidence_intervals) in enumerate(zip(all_ari_scores, all_ari_confidence_intervals)):\n",
    "        ari_scores = np.array(ari_scores)\n",
    "        lower_ci = np.array([ci[0] for ci in ari_confidence_intervals])\n",
    "        upper_ci = np.array([ci[1] for ci in ari_confidence_intervals])\n",
    "\n",
    "        ax_robustness.errorbar(noise_levels, ari_scores,\n",
    "                               yerr=[ari_scores - lower_ci, upper_ci - ari_scores],\n",
    "                               fmt='o-', capsize=5, label=folder_labels[idx])\n",
    "\n",
    "    ax_robustness.set_xlabel('Noise Level', fontsize=22)\n",
    "    ax_robustness.set_ylabel('Adjusted Rand Index', fontsize=22)\n",
    "    ax_robustness.tick_params(axis='both', labelsize=22)\n",
    "    ax_robustness.legend(fontsize=22)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_path, 'second_level_clustering_robustness_analysis_1s.png'), dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved second-level clustering robustness analysis plot to: {os.path.join(output_path, 'second_level_clustering_robustness_analysis_1s.png')}\")\n",
    "    plt.close()\n",
    "\n",
    "    # Save metrics to CSV\n",
    "    csv_filename = os.path.join(output_path, 'clustering_metrics.csv')\n",
    "    with open(csv_filename, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        # Write header\n",
    "        header = ['Subject']\n",
    "        for folder in folder_paths:\n",
    "            folder_name = os.path.basename(folder)\n",
    "            for noise_level in noise_levels:\n",
    "                header.append(f'Rand_{folder_name}_{noise_level:.2f}')\n",
    "        writer.writerow(header)\n",
    "\n",
    "        # Write data\n",
    "        for subject in csv_data:\n",
    "            row = [subject]\n",
    "            for folder in folder_paths:\n",
    "                folder_name = os.path.basename(folder)\n",
    "                for noise_level in noise_levels:\n",
    "                    row.append(csv_data[subject].get(f'Rand_{folder_name}_{noise_level:.2f}', ''))\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f\"Saved clustering metrics to: {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HlF9twyBqPlx",
    "outputId": "34800a13-9753-45c5-ed54-47f9112c35c3"
   },
   "outputs": [],
   "source": [
    "folder_paths = [\n",
    "    './HC/500ms',\n",
    "    './HC/1s',\n",
    "    './HC/5s',\n",
    "    './HC/10s',\n",
    "    './HC/20s',\n",
    "    './HC/30s'\n",
    "]\n",
    "\n",
    "plot_clusters_2d_tsne(folder_paths, n_clusters=4, output_path='./Awake', max_iter=1000, n_perturbations=100, noise_levels=np.linspace(0, 1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3U9lIXXqPl0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
